container: "docker://maxfieldkelsey/te-seq:v1.1"
import os
import pandas as pd
import csv
from pathlib import Path
from pandas.core.common import flatten

sample_table = pd.read_csv("conf/sample_table_aref.csv")

if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
    if config["aref"]["running_ldna"]["response"] == "yes":
        d_or_e = ["default", "extended"]
    else:
        d_or_e = ["default"] 
else:
    d_or_e = ["default"]

def aref_complete_input(wildcards):
    final_input = []
    if config["aref"]["symlink_aref_contents"]["response"] == "yes":
        final_input.append("aref_contents.symlinked.outfile")
        return final_input

    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        if config["aref"]["running_ldna"]["response"] == "yes":
            d_or_e = ["default", "extended"]
        else:
            d_or_e = ["default"] 
    else:
        d_or_e = ["default"]

    paths = [
        "aref/default/A.REF_Analysis/l1element_analysis.rds",
        "aref/default/A.REF_annotations/refseq.complete.gff3.gz.tbi",
        "aref/default/A.REF_annotations/refseq.complete.gtf.gz.tbi",
        "aref/default/A.REF_annotations/refseq.complete.bed.gz.tbi",
        "aref/default/A.REF_annotations/refseq_select.complete.gff3.gz.tbi",
        "aref/default/A.REF_annotations/refseq_select.complete.gtf.gz.tbi",
        "aref/default/A.REF_annotations/refseq_select.complete.bed.gz.tbi",
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.gff3.gz.tbi",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.gtf.gz.tbi",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gff3",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.bed",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_rte_beds/outfile.txt",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.sqlite",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/cytobands.bed",default_or_extended = d_or_e),
    ]

    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        paths.append(expand("aref/qc/{sample_or_ref}/{sample_or_ref}pycoQC.html", sample_or_ref = config["aref"]["samples"]))
        paths.append("aref/qc/multiqc.out")
        # paths.append("aref/results/insertions/analyze_somatic_insertions.rds")

        if config["aref"]["update_ref_with_tldr"]["per_sample"] == "yes":
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds", sample_or_ref = config["aref"]["samples"]))
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds", sample_or_ref = config["aref"]["samples"]))
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds", sample_or_ref = config["aref"]["samples"]))
            paths.append(expand("aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt", sample_or_ref = config["aref"]["samples"]))
            
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt", sample_or_ref = config["aref"]["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gtf", sample_or_ref = config["aref"]["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.gff3", sample_or_ref = config["aref"]["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gff3", sample_or_ref = config["aref"]["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_indeces/make_star_index.out", sample_or_ref = config["aref"]["samples"], default_or_extended = d_or_e))
            if config["aref"]["patch_ref"] == "yes":
                paths.append(expand("aref/{default_or_extended}/{sample_or_ref}.snp_patched_indeces/make_star_index.out", sample_or_ref = config["aref"]["samples"], default_or_extended = d_or_e))
        else:
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds", sample_or_ref = "A.REF"))
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds", sample_or_ref = "A.REF"))
            
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gtf", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.gff3", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gff3", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_indeces/make_star_index.out", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            if config["aref"]["patch_ref"] == "yes":
                paths.append(expand("aref/{default_or_extended}/{sample_or_ref}.snp_patched_indeces/make_star_index.out", sample_or_ref = "A.REF", default_or_extended = d_or_e))

    else:
        paths.append("aref/default/A.REF_indeces/make_star_index.out")
        if config["aref"]["patch_ref"] == "yes":
            paths.append("aref/default/A.REF.snp_patched_indeces/make_star_index.out")


    if config["aref"]["running_ldna"]["response"] == "yes":
        paths.append("aref/extended/A.REF_annotations/cpg_islands.bed")
        if config["aref"]["update_ref_with_tldr"]["per_sample"] == "yes":
            paths.append(expand("aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.vcf", sample_or_ref = config["aref"]["samples"]))
        else:
            paths.append(expand("aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.vcf", sample_or_ref = "A.REF"))
    
    for path in list(flatten(paths)):
        final_input.append(path)
    return final_input


#tag FILESTRUCTURE
paths = [
    "aref/benchmarks",
    ]
if config["aref"]["symlink_aref"]["response"] == "no":
    for path in paths:
        os.makedirs(path, exist_ok = True)

rule all:
    input: ancient("aref.done.outfile")
rule aref_complete:
    input: aref_complete_input
    output: "aref.done.outfile"
    shell: "touch {output}"
rule sym_link:
    input:
        aref_dir = config["aref"]["symlink_aref"]["aref_symlinkdir"]
    priority: 100
    output:
        "aref.done.outfile"
    shell:
        """
ln -s {input.aref_dir}
touch {output}
        """
rule sym_link_aref_contents:
    input:
        aref_dir = config["aref"]["symlink_aref_contents"]["aref_symlinkdir"]
    output:
        sym_link_outfile = "aref_contents.symlinked.outfile"
    shell:
        """
ln -s {input.aref_dir}/* aref/
touch {output.sym_link_outfile}
        """

#tag BASECALLING
checkpoint index_dorado_ref:
    output:
        reference_fai = config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"] + ".fai"
    benchmark:
        "aref/benchmarks/index_dorado_ref.tsv"
    conda: "omics"
    shell:
        """
samtools faidx ${{{output.reference_fai}%.*}}
        """

rule dorado:
    input:
        reference_fai = lambda wildcards: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"] + ".fai"
    output:
        calls = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    container: None
    benchmark:
        "aref/benchmarks/dorado/{rate}.{sample}.{type}.{modifications_string}.tsv"
    params:
        dorado = config["aref"]["dorado"],
        reference = config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        input_dir = lambda wildcards: sample_table.loc[sample_table["sample_name"] == wildcards.sample, "nanopore_rawdata_dir"].iloc[0]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        runtime = 5760,
        slurm_extra="'--gres=gpu:2'"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{params.input_dir} \
--recursive \
--verbose \
--reference {params.reference} > {output.calls}.unfinished
mv {output.calls}.unfinished {output.calls}
        """

rule dorado_unmappedbam:
    output:
        calls = "aref/intermediates/{sample}/unmappedbam/{rate}/{sample}.{type}.{modifications_string}.bam"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    container: None
    benchmark:
        "aref/benchmarks/dorado_fq/{rate}.{sample}.{type}.{modifications_string}.tsv"
    params:
        dorado = config["aref"]["dorado"],
        input_dir = lambda wildcards: sample_table.loc[sample_table["sample_name"] == wildcards.sample, "nanopore_rawdata_dir"].iloc[0]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        runtime = 5760,
        slurm_extra="'--gres=gpu:2'"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{params.input_dir} \
--recursive \
--verbose > {output.calls}.unfinished
mv {output.calls}.unfinished {output.calls}
        """

rule call_dorado_resume:
    input:
        expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.unfinished", sample = config["aref"]["samples"], rate = config["aref"]["rate"], type = config["aref"]["type"], modifications_string = config["aref"]["modification_string"])

rule dorado_resume:
    input:
        unfinished = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.unfinished"
    output:
        calls = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.finished"
    benchmark:
        "aref/benchmarks/dorado_resume/{rate}.{sample}.{type}.{modifications_string}.tsv"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    params:
        dorado = config["aref"]["dorado"],
        basecallingModel = lambda w: config["aref"]["basecallingModel"][w.rate][w.type],
        reference = config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        input_dir = lambda wildcards: sample_table.loc[sample_table["sample_name"] == wildcards.sample, "nanopore_rawdata_dir"].iloc[0]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=48:00:00 --prefer=a6000 --gres=gpu:2"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{params.input_dir} \
--resume-from {input.unfinished} \
--recursive \
--verbose \
--reference {params.reference} > {output.calls}.temp
mv {output.calls}.temp {output.calls}
        """

rule dorado_seqsummary:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
        sortedbamindex = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam.bai", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
    params:
        dorado = config["aref"]["dorado"]
    benchmark:
        "aref/benchmarks/dorado_seqsummary/{sample}.tsv"
    output:
        "aref/qc/{sample}/{sample}.doradosummary.txt"
    conda:
        "omics"
    shell:
        """
{params.dorado} summary {input.sortedbam} > {output}
        """

rule pycoQC:
    input:
        seqsummary = "aref/qc/{sample}/{sample}.doradosummary.txt",
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
    output:
        html = "aref/qc/{sample}/{sample}pycoQC.html",
        json = "aref/qc/{sample}/{sample}pycoQC.json"
    benchmark:
        "aref/benchmarks/pycoQC/{sample}.tsv"
    resources:
        mem_mb = 100000,
    conda:
        "pycoQC"
    shell:
        """
mkdir -p $(dirname {output})
pycoQC --summary_file {input.seqsummary} --bam_file {input.sortedbam} --html_outfile {output.html} --json_outfile {output.json} --min_pass_qual 10
        """

rule multiqc:
    input:
        seqsummary = expand("aref/qc/{sample}/{sample}.doradosummary.txt", sample = config["aref"]["samples"]),
        sortedbam = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam", sample = config["aref"]["samples"], rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
    output:
        mqc = "aref/qc/multiqc.out",
    benchmark:
        "aref/benchmarks/multiqc/multiqc.tsv"
    resources:
        mem_mb = 100000,
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.mqc})
multiqc -f -o $(dirname {output.mqc}) --export ./aref
touch {output.mqc}
        """



#tag ALIGNMENT_UTILITIES
rule subsample_bam:
    input:
        bam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    output:
        subsampled = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.subsampled.bam"
    benchmark:
        "aref/benchmarks/subsample_bam/{rate}.{sample}.{type}.{modifications_string}.tsv"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.subsampled})

samtools view -b {input.bam} chr10 > {output.subsampled}
        """
if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        if config["aref"]["update_ref_with_tldr"]["per_sample"] == "no":
            ruleorder: merge_bams_when_per_sample_is_no_for_ldna_variants > sortBam 
rule merge_bams_when_per_sample_is_no_for_ldna_variants:
    input:
        sorted_bams = expand("aref/intermediates/{sample}/alignments/{{rate}}/{sample}.{{type}}.{{modifications_string}}.sorted.bam", sample = config["aref"]["samples"])
    output:
        merged_bams = "aref/intermediates/A.REF/alignments/{rate}/A.REF.{type}.{modifications_string}.sorted.bam"
    benchmark:
        "aref/benchmarks/sortBam/{rate}.{type}.{modifications_string}.tsv"
    wildcard_constraints:
        modifications_string = "[A-Za-z0-9_-]+"
    resources:
        cpus_per_task =10,
        mem_mb = 128000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output})
samtools merge -@8 {output.merged_bams} {input.sorted_bams}
        """

rule sortBam:
    input:
        bam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    output:
        sortedbam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.sorted.bam"
    benchmark:
        "aref/benchmarks/sortBam/{rate}.{sample}.{type}.{modifications_string}.tsv"
    wildcard_constraints:
        modifications_string = "[A-Za-z0-9_-]+"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output})
samtools sort -@8 -m4g {input.bam} > {output.sortedbam}
        """

rule filterbam:
    input:
        bam = "aref/{path}.sorted.bam"
    params:
        min_mapq = 10,
        min_qscore = 10
    output:
        bam = "aref/{path}.sorted.filtered.bam"
    resources:
        cpus_per_task =6,
        mem_mb = 40000,
        runtime = 300
    conda:
        "omics"
    shell: 
        """
#keeps on only primary and supplementary alignments
samtools view -b -F 0x100 -q {params.min_mapq} -e '[qs] >= {params.min_qscore}' {input.bam} > {output.bam}
        """

rule IndexBam:
    input:
        bam = "aref/{bampath}.bam"
    output:
        index = "aref/{bampath}.bam.bai"
    benchmark:
        "aref/benchmarks/IndexBam/{bampath}.tsv"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
samtools index  -@6 {input.bam}
        """
        
species = config["aref"]["species"]
rule tldr_aggregate_multiple_samples:
    input:
        bams = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.filtered.bam", sample = config["aref"]["samples"], rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
        bais = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.filtered.bam.bai", sample = config["aref"]["samples"], rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
    params:
        tldr_input_bam_ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldr_te_ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_te_ref"][species],
        known_nonref = lambda w: [config["aref"]["update_ref_with_tldr"]["known_nonref"]["path"] if config["aref"]["update_ref_with_tldr"]["known_nonref"]["response"] == "yes" else "not_provided"][0]
    benchmark:
        "aref/benchmarks/tldr_aggregate_multiple_samples/tldr_aggregate_multiple_samples.tsv"
    output:
        tldr = "aref/A.REF_tldr/A.REF.table.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda:
        "tldr"
    shell:
        """
bams=$(echo {input.bams})
commabams=$(echo $bams | tr ' ' ',')

echo "params known nonref"
nonrefavailible={params.known_nonref}
echo $nonrefavailible
if [[ $nonrefavailible = "not_provided" ]]
then

tldr -b $commabams \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--minreads 1 \
--outbase A.REF \
--detail_output \
--extend_consensus 4000 \
--trdcol

else

tldr -b $commabams \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--nonref {params.known_nonref} \
--minreads 1 \
--outbase A.REF \
--detail_output \
--extend_consensus 4000 \
--trdcol

fi

mkdir -p $(dirname {output.tldr})
mv A.REF.table.txt {output.tldr}
mv A.REF $(dirname {output.tldr})/
        """

rule call_tldr_per_sample:
    input:
        expand("aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt", sample = config["aref"]["samples"], sample_or_ref = config["aref"]["samples"])
rule tldr_per_sample:
    input:
        bam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.filtered.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
        bai = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.filtered.bam.bai", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"])
    benchmark:
        "aref/benchmarks/tldr_per_sample/{sample}.tsv"
    params:
        tldr_input_bam_ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldr_te_ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_te_ref"][species],
        known_nonref = lambda w: [config["aref"]["update_ref_with_tldr"]["known_nonref"]["path"] if config["aref"]["update_ref_with_tldr"]["known_nonref"]["response"] == "yes" else "not_provided"][0]
    output:
        tldr = "aref/{sample}_tldr/{sample}.table.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda:
        "tldr"
    shell:
        """
echo "params known nonref"
nonrefavailible={params.known_nonref}
echo $nonrefavailible
if [[ $nonrefavailible = "not_provided" ]]
then

tldr -b {input.bam} \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--minreads 1 \
--outbase {wildcards.sample} \
--detail_output \
--extend_consensus 4000 \
--trdcol

else

tldr -b {input.bam} \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--nonref {params.known_nonref} \
--minreads 1 \
--outbase {wildcards.sample} \
--detail_output \
--extend_consensus 4000 \
--trdcol

fi

mkdir -p $(dirname {output.tldr})
mv {wildcards.sample}.table.txt {output.tldr}
mv {wildcards.sample} $(dirname {output.tldr})/
        """

rule update_reference:
    input:
        reference = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        updated_reference = "aref/default/{sample_or_ref}-pre-ins-filtering.fa",
        non_ref_contigs = "aref/default/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa",
    benchmark:
        "aref/benchmarks/update_reference/{sample_or_ref}.tsv"
    conda:
        "repeatanalysis"
    script:
        "scripts/create_reference.R"

rule update_reference_extended_nonref_flank:
    input:
        reference = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        updated_reference_plusflank = "aref/extended/{sample_or_ref}-pre-ins-filtering.fa",
        non_ref_contigs_plusflank = "aref/extended/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa"
    benchmark:
        "aref/benchmarks/update_reference/{sample_or_ref}.tsv"
    conda:
        "repeatanalysis"
    script:
        "scripts/create_reference_extended_nonref_flank.R"

if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        if config["aref"]["update_ref_with_tldr"]["per_sample"] == "yes":
            ruleorder: copy_starting_reference > cleanup_updated_ref > cleanup_updated_ref_and_patch

if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        if config["aref"]["patch_ref"] == "yes":
            ruleorder: cleanup_updated_ref_and_patch > cleanup_updated_ref
            ruleorder: cleanup_updated_sample_ref_and_patch > cleanup_updated_sample_ref
        else:
            ruleorder:  cleanup_updated_ref > cleanup_updated_ref_and_patch
            ruleorder:  cleanup_updated_sample_ref > cleanup_updated_sample_ref_and_patch
rule copy_starting_reference:
    params:
        ref = config["aref"]["starting_ref"]
    conda:
        "omics"
    priority: 100
    output:
        ref = "aref/{default_or_extended}/A.REF.fa"
    shell:
        """
cp {params.ref} {output.ref}
samtools faidx {output.ref}
        """
       
rule cleanup_updated_ref:
    input:
        updated_reference = "aref/{default_or_extended}/A.REF-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{default_or_extended}/A.REF_contigs_to_keep.txt"
    output:
        filtered_ref = "aref/{default_or_extended}/A.REF.fa"
    priority: 100
    benchmark:
        "aref/benchmarks/cleanup_updated_ref/cleanup_updated_ref_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
        """

rule cleanup_updated_sample_ref:
    input:
        updated_reference = "aref/{default_or_extended}/{sample}-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{default_or_extended}/{sample}_contigs_to_keep.txt"
    output:
        filtered_ref = "aref/{default_or_extended}/{sample}.fa"
    priority: 100
    benchmark:
        "aref/benchmarks/cleanup_updated_sample_ref/{sample}_{default_or_extended}.tsv"
    wildcard_constraints:
        sample = "[A-Za-z0-9_]+"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
samtools faidx {output.filtered_ref}
        """

rule cleanup_updated_ref_and_patch:
    input:
        updated_reference = "aref/{default_or_extended}/A.REF-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{default_or_extended}/A.REF_contigs_to_keep.txt",
        hq_snps = "aref/intermediates/A.REF/pepper/hq_snps.vcf.gz",
    output:
        filtered_ref = "aref/{default_or_extended}/A.REF.snp_patched.fa"
    priority: 100
    benchmark:
        "aref/benchmarks/cleanup_updated_ref_and_patch/cleanup_updated_ref_and_patch_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}.temp
bcftools consensus -I -f {output.filtered_ref}.temp -o {output.filtered_ref} {input.hq_snps}
rm {output.filtered_ref}.temp
samtools faidx {output.filtered_ref}
        """

rule cleanup_updated_sample_ref_and_patch:
    input:
        updated_reference = "aref/{default_or_extended}/{sample}-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{default_or_extended}/{sample}_contigs_to_keep.txt",
        hq_snps = "aref/intermediates/{sample}/pepper/hq_snps.vcf.gz",
    output:
        filtered_ref = "aref/{default_or_extended}/{sample}.snp_patched.fa"
    priority: 100
    benchmark:
        "aref/benchmarks/cleanup_updated_sample_ref_and_patch/{sample}_{default_or_extended}.tsv"
    wildcard_constraints:
        sample = "[A-Za-z0-9_]+"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
bcftools consensus -I -f {output.filtered_ref}.temp -o {output.filtered_ref} {input.hq_snps}
rm {output.filtered_ref}.temp
samtools faidx {output.filtered_ref}
        """

#Variant calling for ref patching
rule sniffles:
    input:
        sortedbam = expand("aref/intermediates/{{sample_or_ref}}/alignments/{rate}/{{sample_or_ref}}.{type}.{modification_string}.sorted.filtered.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
        sortedbamindex = expand("aref/intermediates/{{sample_or_ref}}/alignments/{rate}/{{sample_or_ref}}.{type}.{modification_string}.sorted.filtered.bam.bai", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"])
    output:
        vcf = "aref/intermediates/{sample_or_ref}/sniffles/sniffles.vcf"
    params:
        ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"]
    resources:
        runtime = 1000,
        mem_mb = 128000,
        cpus_per_task = 32
    conda: "sniffles"
    container: None
    shell:
        """
mkdir -p $(dirname {output})
sniffles -i {input.sortedbam} -v {output.vcf} --threads 28 --mosaic --reference {params.ref}
        """

rule pepper:
    input:
        sortedbam = ancient(expand("aref/intermediates/{{sample_or_ref}}/alignments/{rate}/{{sample_or_ref}}.{type}.{modification_string}.sorted.filtered.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"])),
        sortedbamindex = ancient(expand("aref/intermediates/{{sample_or_ref}}/alignments/{rate}/{{sample_or_ref}}.{type}.{modification_string}.sorted.filtered.bam.bai", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]))
    params:
        ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
    output:
        outfile = "aref/intermediates/{sample_or_ref}/pepper/{chr}/pepper.out.outfile",
        vcf = "aref/intermediates/{sample_or_ref}/pepper/{chr}/PEPPER_MARGIN_DEEPVARIANT_FINAL_OUTPUT.vcf.gz"

    resources:
        cpus_per_task =20,
        runtime = 600,
        mem_mb = 75000
    container:
        "docker://kishwars/pepper_deepvariant:r0.8"
    shell:	
        """
mkdir -p $(dirname {output.outfile})
touch {output.outfile}

run_pepper_margin_deepvariant --help
run_pepper_margin_deepvariant call_variant \
-b {input.sortedbam} \
-f {params.ref} \
-o $(dirname {output.outfile}) \
-t {resources.cpus_per_task} \
-r {wildcards.chr} \
--pepper_min_mapq 30 \
--pepper_snp_p_value 0.2 \
--pepper_insert_p_value 0.3 \
--pepper_delete_p_value 0.3 \
--ont_r10_q20
        """

rule get_reference_gatk_dict:
    params:
        ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        refdict = lambda w: "%s.dict"%config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"]
    output:
        "aref/intermediates/ref_gatk_dict.outfile"
    container:
        "docker://broadinstitute/gatk:4.6.1.0"
    shell:
        """
gatk CreateSequenceDictionary \
    -R {params.ref} \
    -O {params.refdict}
mkdir -p $(dirname {output})
touch {output}
        """


rule liftover_dbsnp:
    input:
        refdict = lambda w: "%s.dict"%config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"]
    params:
        vcf = "/oscar/data/jsedivy/mkelsey/REF_STABLE_ANNOTATIONS/HS1/downloaded_annotaions/GCF_000001405.40.ucsc.vcf",
        chain = "/oscar/data/jsedivy/mkelsey/ref/genomes/hs1/liftOver/hg38-chm13v2.over.chain",
        ref = lambda w: config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"],
        ref_gatk_dict = "aref/intermediates/ref_gatk_dict.outfile"
    output:
        "/oscar/data/jsedivy/mkelsey/REF_STABLE_ANNOTATIONS/HS1/downloaded_annotaions/hs1_GCF_000001405.40.ucsc.vcf"
    container:
        "docker://broadinstitute/gatk:4.6.1.0"
    priority: 100
    resources:
        cpus_per_task =1,
        runtime = 600,
        mem_mb = 80000
    shell:
        """
gatk LiftoverVcf -I {params.vcf} -C {params.chain} -O {output} -R {params.ref} --REJECT {output}.rejected.vcf
        """


# rule pepper_hg38:
#     input:
#         sortedbam = ancient(expand("ldna/intermediates/{{sample_or_ref}}/alignments/{rate}/hg38/{{sample_or_ref}}.{type}.{modification_string}.sorted.filtered.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"])),
#         sortedbamindex = ancient(expand("ldna/intermediates/{{sample_or_ref}}/alignments/{rate}/hg38/{{sample_or_ref}}.{type}.{modification_string}.sorted.filtered.bam.bai", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]))
#     params:
#         ref = "/oscar/data/jsedivy/mkelsey/ref/genomes/hg38/hg38.p13.sorted.fa",
#     output:
#         outfile = "aref/intermediates/{sample_or_ref}/pepper/hg38/{chr}/pepper.out.outfile",
#         vcf = "aref/intermediates/{sample_or_ref}/pepper/hg38/{chr}/PEPPER_MARGIN_DEEPVARIANT_FINAL_OUTPUT.vcf.gz"
#     resources:
#         cpus_per_task =20,
#         runtime = 600,
#         mem_mb = 50000
#     container:
#         "docker://kishwars/pepper_deepvariant:r0.8"
#     shell:	
#         """
# mkdir -p $(dirname {output.outfile})
# touch {output.outfile}

# run_pepper_margin_deepvariant --help
# run_pepper_margin_deepvariant call_variant \
# -b {input.sortedbam} \
# -f {params.ref} \
# -o $(dirname {output.outfile}) \
# -t {resources.cpus_per_task} \
# -r {wildcards.chr} \
# --pepper_min_mapq 30 \
# --pepper_snp_p_value 0.2 \
# --pepper_insert_p_value 0.3 \
# --pepper_delete_p_value 0.3 \
# --ont_r10_q20
#         """


# rule merge_vcfs_hg38:
#     input:
#         vcfs = [expand("aref/intermediates/{{sample_or_ref}}/pepper/hg38/{chromosome}/PEPPER_MARGIN_DEEPVARIANT_FINAL_OUTPUT.vcf.gz", chromosome = [e for e in pd.read_table(config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"] + ".fai", header = None).iloc[:,0] if e != "chrM"]) if config["aref"]["update_ref_with_tldr"]["response"] == "yes" else ""] #this used to thrown an error if the tldr_input_bam_ref fai did not exist - even if you weren't running the tldr
#     output:
#         vcf = "aref/intermediates/{sample_or_ref}/pepper/hg38/merged.vcf.gz"
#     conda:
#         "omics"
#     shell:
#         """
# bcftools concat --naive {input.vcfs} > {output.vcf}.temp
# bcftools sort {output.vcf}.temp > {output.vcf}.temp.sorted
# bgzip -c {output.vcf}.temp.sorted > {output.vcf}.temp.gz
# mv {output.vcf}.temp.gz {output.vcf}
# tabix {output.vcf}
# rm {output.vcf}.temp
# rm {output.vcf}.temp.sorted
#         """


# rule extract_highquality_snps_hg38:
#     input:
#         vcf = "aref/intermediates/{sample_or_ref}/pepper/hg38/merged.vcf.gz"
#     output:
#         hq_snps = "aref/intermediates/{sample_or_ref}/pepper/hg38/hq_snps.vcf.gz"
#     conda:
#         "omics"
#     shell:
#         """
# bcftools filter -i 'TYPE="snp" && MIN(DP)>10 && QUAL>20 && FILTER="PASS"' {input.vcf} > {output.hq_snps}
# bgzip {output.hq_snps}
# mv {output.hq_snps}.gz {output.hq_snps}
# tabix {output.hq_snps}
#         """

# rule snpeff_smv_snps_hg38:
#     input: 
#         vcf = "aref/intermediates/{sample_or_ref}/pepper/hg38/merged.vcf.gz"
#     resources:
#         runtime = 1000,
#         mem_mb = 128000,
#         cpus_per_task = 12
#     conda: "snpeff"
#     container: None
#     output:
#         outfile = "aref/intermediates/{sample_or_ref}/snpeff/hg38/snpeff_smv.outfile",
#         vcf = "aref/intermediates/{sample_or_ref}/snpeff/hg38/snpeff_sml.vcf"
#     shell:
#         """
# zcat {input.vcf} | java -Xmx60g -jar $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.jar hg38 -v -c $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.config -chr 'chr' -csvStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/csvStats_sml.csv -htmlStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/snpeffStats_sml.html - > {output.vcf}
# mkdir -p $(dirname {output.outfile})
# touch {output.outfile}
# exit 0
#         """

# rule snpeff_sv_hg38:
#     input: 
#         vcf = "aref/intermediates/{sample_or_ref}/sniffles/sniffles.vcf"
#     resources:
#         runtime = 1000,
#         mem_mb = 128000,
#         cpus_per_task = 12
#     conda: "snpeff"
#     container: None
#     output:
#         outfile = "aref/intermediates/{sample_or_ref}/snpeff/hg38/snpeff_sv.outfile",
#         vcf = "aref/intermediates/{sample_or_ref}/snpeff/hg38/snpeff_sv.vcf"
#     shell:
#         """
# cat {input.vcf} | java -Xmx60g -jar $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.jar hs1 -v -c $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.config -chr 'chr' -csvStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/csvStats_sv.csv -htmlStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/snpeffStats_sv.html {input.vcf} > {output.vcf}
# mkdir -p $(dirname {output.outfile})
# touch {output.outfile}
# exit 0
#         """

######### 
def merge_vcfs_input(wildcards):
    #call checkpoint output just to register rule merge_vcfs as dependent on the move_staring_reference rule
    checkpoint_output = checkpoints.index_dorado_ref.get().output[0]
    if os.path.exists(config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"] + ".fai"):
        return expand("aref/intermediates/{{sample_or_ref}}/pepper/{chromosome}/PEPPER_MARGIN_DEEPVARIANT_FINAL_OUTPUT.vcf.gz", chromosome = [e for e in pd.read_table(config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"] + ".fai", header = None).iloc[:,0] if e != "chrM"])
    else:
        return []
rule merge_vcfs:
    input:
        vcfs = merge_vcfs_input
    output:
        vcf = "aref/intermediates/{sample_or_ref}/pepper/merged.vcf.gz"
    conda:
        "omics"
    shell:
        """
bcftools concat --naive {input.vcfs} > {output.vcf}.temp
bcftools sort {output.vcf}.temp > {output.vcf}.temp.sorted
bgzip -c {output.vcf}.temp.sorted > {output.vcf}.temp.gz
mv {output.vcf}.temp.gz {output.vcf}
tabix {output.vcf}
rm {output.vcf}.temp
rm {output.vcf}.temp.sorted
        """


rule extract_highquality_snps:
    input:
        vcf = "aref/intermediates/{sample_or_ref}/pepper/merged.vcf.gz"
    output:
        hq_snps = "aref/intermediates/{sample_or_ref}/pepper/hq_snps.vcf.gz"
    conda:
        "omics"
    shell:
        """
bcftools filter -i 'TYPE="snp" && MIN(DP)>10 && QUAL>20 && FILTER="PASS"' {input.vcf} > {output.hq_snps}
bgzip {output.hq_snps}
mv {output.hq_snps}.gz {output.hq_snps}
tabix {output.hq_snps}
        """


rule snpeff_build_db:
    input:
        ref = config["ldna"]["reference"]
    params:
        refseq = config["aref"]["ref_refseq_gtf"]
    output:
        outfile = "aref/intermediates/snpeff/db.outfile"
    resources:
        runtime = 1000,
        mem_mb = 128000,
        cpus_per_task = 12
    conda: "snpeff"
    container: None
    shell:
        """
configfile=$(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.config
echo '# Human genome, version hs1
hs1.genome : Human' >> $configfile
mkdir -p $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/data/hs1
cp {params.refseq} $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/data/hs1/genes.gtf
cp {input.ref} $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/data/hs1/sequences.fa

java -Xmx60g -jar $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.jar build -gtf22 -v hs1 -noCheckCds -noCheckProtein
mkdir -p $(dirname {output.outfile})
touch {output.outfile}
        """

rule snpeff_smv:
    input: 
        vcf = "aref/intermediates/{sample_or_ref}/pepper/merged.vcf.gz",
        db_outfile = "aref/intermediates/snpeff/db.outfile"
    resources:
        runtime = 1000,
        mem_mb = 128000,
        cpus_per_task = 12
    conda: "snpeff"
    container: None
    output:
        outfile = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_smv.outfile",
        vcf = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.vcf"
    shell:
        """
zcat {input.vcf} | java -Xmx60g -jar $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.jar hs1 -v -c $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.config -chr 'chr' -csvStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/csvStats_sml.csv -htmlStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/snpeffStats_sml.html - > {output.vcf}
mkdir -p $(dirname {output.outfile})
touch {output.outfile}
exit 0
        """

rule snpeff_sv:
    input: 
        vcf = "aref/intermediates/{sample_or_ref}/sniffles/sniffles.vcf",
        db_outfile = "aref/intermediates/snpeff/db.outfile"
    resources:
        runtime = 1000,
        mem_mb = 128000,
        cpus_per_task = 12
    conda: "snpeff"
    container: None
    output:
        outfile = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.outfile",
        vcf = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.vcf"
    shell:
        """
mkdir -p $(dirname {output.outfile})
java -Xmx60g -jar $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.jar hs1 -v -c $(dirname $(dirname $(which snpEff)))/share/snpeff-5.2-0/snpEff.config -chr 'chr' -csvStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/csvStats_sv.csv -htmlStats aref/intermediates/{wildcards.sample_or_ref}/snpeff/snpeffStats_sv.html {input.vcf} > {output.vcf}
touch {output.outfile}
exit 0
        """

rule pass_vcf:
    input:
        vcf_sml = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.vcf",
        vcf_sv = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.vcf"
    output:
        vcf_sml = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.vcf",
        vcf_sv = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.pass.vcf"
    conda:
        "omics"
    shell:
        """
bcftools view -f 'PASS' {input.vcf_sml} > {output.vcf_sml}
bcftools view -f 'PASS' {input.vcf_sv} > {output.vcf_sv}
        """


rule annotate_vcf_with_dbsnp:
    input:
        vcf_sml = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.vcf"
    output:
        annvcf = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.ann.vcf.gz",
    params:
        dbsnp = "/oscar/data/jsedivy/mkelsey/REF_STABLE_ANNOTATIONS/HS1/downloaded_annotaions/chm13v2.0_dbSNPv155.vcf.gz",
        dbsnpindex = "/oscar/data/jsedivy/mkelsey/REF_STABLE_ANNOTATIONS/HS1/downloaded_annotaions/chm13v2.0_dbSNPv155.vcf.gz.tbi"
    conda:
        "omics"
    shell:
        """
if [ ! -f {input.vcf_sml}.gz ]; then
    bgzip {input.vcf_sml}
fi
if [ ! -f {input.vcf_sml}.gz.tbi ]; then
    tabix -p vcf {input.vcf_sml}.gz
fi
vcfout={output.annvcf}
bcftools annotate -a {params.dbsnp} -c ID -o ${{vcfout%.*}} -O v {input.vcf_sml}.gz
bgzip --stdout ${{vcfout%.*}} > $vcfout
tabix -p vcf $vcfout
        """

rule getstuff:
    input:
        [
        expand("aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.ann.vcf.gz", sample_or_ref = config["aref"]["samples"])
        ]


rule vcf_to_table:
    input:
        vcf_sml = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.vcf",
        vcf_sv = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.pass.vcf"
    output:
        vcf_sml = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sml.pass.txt",
        vcf_sv = "aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.pass.txt"
    container:
        "docker://broadinstitute/gatk:3.8"
    shell:
        """
bcftools view -f 'PASS' {input.vcf_sml} > {output.vcf_sml}
bcftools view -f 'PASS' {input.vcf_sv} > {output.vcf_sv}
        """

rule getstuff2:
    input:
        [
        expand("aref/intermediates/{sample_or_ref}/snpeff/snpeff_sv.pass.txt", sample_or_ref = config["aref"]["samples"])
        ]

####

rule get_repeatmasker_raw:
    input:
        out = "aref/{default_or_extended}/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    output:
        repeatmasker = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/A.REF_repeatmasker_raw.gtf"
    benchmark:
        "aref/benchmarks/get_repeatmasker_raw/{sample_or_ref}_{default_or_extended}.tsv"
    shell:
        """
cp {input.out} {output.repeatmasker}
workflow/aref/scripts/outToGtf.sh {input.out} {output.repeatmasker}.unsorted
sort -k1,1V -k4,4n -k5,5n {output.repeatmasker}.unsorted > {output.repeatmasker}
rm {output.repeatmasker}.unsorted
        """

checkpoint index_reference:
    input:
        reference = "aref/{default_or_extended}/{sample_or_ref}.fa"
    output:
        reference_index = "aref/{default_or_extended}/{sample_or_ref}.fa.fai"
    benchmark:
        "aref/benchmarks/index_reference/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
samtools faidx {input.reference}
        """

#### to repeat mask or not to repeatmask, that is the question
rule get_repeatmasker_raw_out:
    params:
        rmref = config["aref"]["run_repeatmasker"]["starting_ref_repeatmasker"]
    output:
        out = "aref/{default_or_extended}/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    shell:
        """
cp {params.rmref} {output.out}
        """

rule split_genome_fa_into_chr:
    input: 
        fasta = "aref/default/A.REF.fa",
        fai = "aref/default/A.REF.fa.fai"
    output:
        split = "aref/default/A.REF_split/A.REF_{chromosome}.fa"
    benchmark:
        "aref/benchmarks/split_genome_fa_into_chr/split_genome_fa_into_chr.{chromosome}.tsv"
    conda: "omics"
    shell:
        """
echo {wildcards.chromosome} > {output.split}.names.txt
seqtk subseq {input.fasta} {output.split}.names.txt > {output.split}
        """

rule repeatmasker_whole_genome:
    input: 
        split = "aref/default/A.REF_split/A.REF_{chromosome}.fa"
    params:
        species = config["aref"]["species"]
    benchmark:
        "aref/benchmarks/repeatmasker_whole_genome/repeatmasker_whole_genome.{chromosome}.tsv"
    output:
        rmout = "aref/default/A.REF_repeatmasker/wholegenome/A.REF_{chromosome}.fa.out"
    resources:
        cpus_per_task =20,
        runtime = 1000,
        mem_mb = 30000
    container: "docker://dfam/tetools:1.88.5"
    shell:
        """
if [[ {params.species} == mouse ]]
then
species="Mus musculus"
else
species={params.species}
fi
mkdir -p $(dirname {output})
RepeatMasker -species "$species" -pa {resources.cpus_per_task} -gff {input.split} -dir $(dirname {output})
        """

def merge_outs_wholegenome_input(wildcards):
    #call checkpoint output just to register rule merge_rm_out_wholegenome as dependent on the move_staring_reference rule
    checkpoint_output = checkpoints.index_reference.get().output[0]
    if os.path.exists("aref/default/A.REF.fa.fai"):
        return expand("aref/default/A.REF_repeatmasker/wholegenome/A.REF_{chromosome}.fa.out", chromosome = pd.read_table("aref/default/A.REF.fa.fai", header = None)[0].tolist())
    else:
        return "TEMP"
rule merge_rm_out_wholegenome:
    input:
        outs = merge_outs_wholegenome_input
    benchmark:
        "aref/benchmarks/merge_rm_out_wholegenome/merge_rm_out_wholegenome.tsv"
    output:
        out = "aref/default/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    priority: 99
    conda:
        "evo2"
    shell:
        """
awk '{{ if (NR > 3) exit }} \
    NR < 4' {input.outs} > {output.out}
awk 'FNR > 3' {input.outs} >> {output.out}
        """


#######
#non ref contig buisness
def repeatmasker_input(wildcards):
    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        return expand("aref/{default_or_extended}/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa", sample_or_ref = wildcards.sample_or_ref, default_or_extended = wildcards.default_or_extended)
    else:
        return config["aref"]["ref"]

rule repeatmasker:
    input:
        fasta = repeatmasker_input
    params:
        species = config["aref"]["species"]
    benchmark:
        "aref/benchmarks/repeatmasker/{sample_or_ref}_{default_or_extended}.tsv"
    output:
        rmout = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa.out"
    resources:
        cpus_per_task =20,
        runtime = 1200,
        mem_mb = 50000
    container: "docker://dfam/tetools:1.88.5"
    shell:
        """
if [[ {params.species} == mouse ]]
then
species="Mus musculus"
else
species={params.species}
fi
mkdir -p $(dirname {output})
RepeatMasker -species "$species" -pa {resources.cpus_per_task} -gff {input.fasta} -dir $(dirname {output})
        """
if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        if config["aref"]["update_ref_with_tldr"]["per_sample"] == "yes":
            ruleorder: get_repeatmasker_raw > getGtfs
rule getGtfs:
    input:
        rmout = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa.out",
        rmref = "aref/default/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    params:
        module_prefix = config["aref"]["prefix"]
    benchmark:
        "aref/benchmarks/getGtfs/{sample_or_ref}_{default_or_extended}.tsv"
    output:
       ref = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/{sample_or_ref}_repeatmasker_ref_raw.gtf",
       nonref = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/{sample_or_ref}_repeatmasker_nonref_raw.gtf",
       merged = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/{sample_or_ref}_repeatmasker_raw.gtf"
    conda:
        "evo2"
    shell:
        """
workflow/{params.module_prefix}/scripts/outToGtf.sh {input.rmout} {output.nonref}
workflow/{params.module_prefix}/scripts/outToGtf.sh {input.rmref} {output.ref}
cat {output.ref} {output.nonref} > {output.merged}
        """

if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["update_ref_with_tldr"]["response"] == "yes":
        if config["aref"]["update_ref_with_tldr"]["per_sample"] == "yes":
            ruleorder: process_gtf > process_gtf_tldr
rule process_gtf:
    input:
        gtf = "aref/{default_or_extended}/A.REF_repeatmasker/A.REF_repeatmasker_raw.gtf",
        ref_cytobands = "aref/{default_or_extended}/A.REF_annotations/cytobands.bed",
        ref = "aref/{default_or_extended}/A.REF.fa",
        reffai = "aref/{default_or_extended}/A.REF.fa.fai"
    params:
        tldr_switch = "process_gtf"
    output:
        repmask_gff2 = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.gff2",
        repmask_gff3 = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.gff3",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.gtf.rformatted.fragmentsjoined.csv"
    benchmark:
        "aref/benchmarks/process_gtf/process_gtf_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/process_gtf_tldr.R"

rule process_gtf_tldr:
    input:
        gtf = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/{sample_or_ref}_repeatmasker_raw.gtf",
        ref_cytobands = "aref/{default_or_extended}/A.REF_annotations/cytobands.bed",
        ref = "aref/{default_or_extended}/{sample_or_ref}-pre-ins-filtering.fa",
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    params:
        tldr_switch = "process_gtf_tldr"
    output:
        contigs_to_keep = "aref/{default_or_extended}/{sample_or_ref}_contigs_to_keep.txt",
        filtered_tldr = "aref/{default_or_extended}/{sample_or_ref}.table.kept_in_updated_ref.txt",
        repmask_gff2 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff2",
        repmask_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff3",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv"
    benchmark:
        "aref/benchmarks/process_gtf_tldr/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/process_gtf_tldr.R"

rule analyze_insertions:
    input:
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt",
        filtered_tldr = "aref/{default_or_extended}/{sample_or_ref}.table.kept_in_updated_ref.txt",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{default_or_extended}/{sample_or_ref}-pre-ins-filtering.fa",
        element_analysis = "aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds"
    output:
        plots = "aref/{default_or_extended}/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds",
    benchmark:
        "aref/benchmarks/analyze_insertions/{sample_or_ref}_{default_or_extended}/.tsv"
    conda:
        "evo2"
    script:
        "scripts/analyze_insertions.R"

rule analyze_somatic_insertions:
    input:
        json = expand("aref/qc/{sample}/{sample}pycoQC.json", sample = config["aref"]["samples"]),
        tldroutput = lambda w: expand("aref/{sample}_tldr/{sample}.table.txt", sample = config["aref"]["samples"]) if config["aref"]["update_ref_with_tldr"]["per_sample"] == "yes" else "aref/A.REF_tldr/A.REF.table.txt"
    output:
        plots = "aref/results/insertions/analyze_somatic_insertions.rds"
    benchmark:
        "aref/benchmarks/analyze_somatic_insertions/analyze_somatic_insertions.tsv"
    conda:
        "evo2"
    script:
        "scripts/analyze_somatic_insertions.R"


rule move_refseq:
    input:
        ref_refseq_gff3 = config["aref"]["ref_refseq_gff3"],
        ref_refseq_gtf = config["aref"]["ref_refseq_gtf"]
    output:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gtf"
    benchmark:
        "aref/benchmarks/move_refseq/{sample_or_ref}_{default_or_extended}.tsv"
    shell:
        """
cp {input.ref_refseq_gff3} {output.refseq_gff3}
cp {input.ref_refseq_gtf} {output.refseq_gtf}
        """

rule sort_refseq_get_bed:
    input:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gtf"
    output:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.gtf",
        refseq_bed = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.bed"
    benchmark:
        "aref/benchmarks/sort_refseq_get_bed/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
awk '!/#/ {{print}}' {input.refseq_gff3} | sort -k1,1V -k4,4n -k5,5n > {output.refseq_gff3}
awk '!/#/ {{print}}' {input.refseq_gtf} | sort -k1,1V -k4,4n -k5,5n > {output.refseq_gtf}
gtf2bed < {output.refseq_gtf} > {output.refseq_bed}
        """

rule complete_gff3:
    input:
        gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.gff3",
    output:
        complete_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    benchmark:
        "aref/benchmarks/complete_gff3/{sample_or_ref}.{annotation}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
rm -f {output.complete_gff3}.temp
agat_convert_sp_gxf2gxf.pl --gff {input.gff3} -o {output.complete_gff3}.temp
awk '!/#/ {{print}}' {output.complete_gff3}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.complete_gff3}
rm  {output.complete_gff3}.temp
        """

rule gff_to_gtf:
    input:
        complete_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    output:
        gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gtf",
    benchmark:
        "aref/benchmarks/gff_to_gtf/{sample_or_ref}.{annotation}_{default_or_extended}.tsv"
    conda:
        "omics"
    resources:
        mem_mb = 128000,
        runtime = 300
    shell:
        """
rm -f {output.gtf}.temp
agat_convert_sp_gff2gtf.pl --gff {input.complete_gff3} -o {output.gtf}.temp
awk '!/#/ {{print}}' {output.gtf}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.gtf}
rm {output.gtf}.temp
        """

rule gff_to_bed12:
    input:
        complete_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    output:
        bed12 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.bed"
    benchmark:
        "aref/benchmarks/gff_to_bed12/{sample_or_ref}.{annotation}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
rm -f {output.bed12}.temp
agat_convert_sp_gff2bed.pl --gff {input.complete_gff3} -o {output.bed12}.temp
awk '!/#/ {{print}}' {output.bed12}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k2,2n -k3,3n > {output.bed12}
rm {output.bed12}.temp
        """
if config["aref"]["symlink_aref"]["response"] == "no":
    ruleorder: get_complete_refseq_select > gff_to_gtf 
    ruleorder: get_complete_refseq_select > gff_to_bed12 

rule get_complete_refseq_select:
    input:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.complete.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.complete.gtf",
        refseq_bed = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.complete.bed"
    output:
        refseq_select_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq_select.complete.gff3",
        refseq_select_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq_select.complete.gtf",
        refseq_select_bed = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq_select.complete.bed",
    benchmark:
        "aref/benchmarks/get_complete_refseq_select/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
awk '/tag=RefSeq Select/ {{print}}' {input.refseq_gff3} > {output.refseq_select_gff3}
awk '/tag=RefSeq Select/ {{print}}' {input.refseq_bed} > {output.refseq_select_bed}
awk '/tag=RefSeq Select/ {{print}}' {input.refseq_gtf} > {output.refseq_select_gtf}
        """
if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["symlink_aref"]["response"] == "no":
        ruleorder: merge_genes_and_repeats_gff > complete_gff3 
rule merge_genes_and_repeats_gff:
    input:
        complete_repeat_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.complete.gff3",
        complete_refseq_gff3 = "aref/default/A.REF_annotations/refseq.complete.gff3"
    output:
        merged_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gff3"
    benchmark:
        "aref/benchmarks/merge_genes_and_repeats_gff/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 600
    conda:
        "omics"
    shell:
        """
rm -f {output.merged_gff3}.temp
agat_sp_merge_annotations.pl -f {input.complete_refseq_gff3} -f {input.complete_repeat_gff3} -o {output.merged_gff3}.temp
awk '!/#/ {{print}}' {output.merged_gff3}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n >  {output.merged_gff3}
rm {output.merged_gff3}.temp
        """

rule merge_OG_genes_and_repeats_gff3:
    input:
        repeatmasker = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff3",
        refseq = "aref/default/A.REF_annotations/refseq.gff3"
    output:
        merged_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.gff3"
    benchmark:
        "aref/benchmarks/merge_OG_genes_and_repeats_gff3/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
rm -f {output.merged_gff3}.temp
awk '!/#/ {{print}}' {input.repeatmasker} | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' > {output.merged_gff3}.temp
awk '!/#/ {{print}}' {input.refseq} | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' >> {output.merged_gff3}.temp
cat {output.merged_gff3}.temp | sort -k1,1V -k4,4n -k5,5n > {output.merged_gff3}
rm {output.merged_gff3}.temp
        """
        
if config["aref"]["symlink_aref"]["response"] == "no":
    if config["aref"]["symlink_aref"]["response"] == "no":
        ruleorder: merge_genes_and_repeats_gtf > gff_to_gtf 

rule merge_genes_and_repeats_gtf:
    input:
        complete_repeat_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.complete.gtf",
        complete_refseq_gtf = "aref/default/A.REF_annotations/refseq.complete.gtf"
    output:
        merged_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gtf"
    benchmark:
        "aref/benchmarks/merge_genes_and_repeats_gtf/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 600
    conda:
        "omics"
    shell:
        """
rm -f {output.merged_gtf}.temp
agat_sp_merge_annotations.pl -f {input.complete_refseq_gtf} -f {input.complete_repeat_gtf} -o {output.merged_gtf}.temp
awk '!/#/ {{print}}' {output.merged_gtf}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.merged_gtf}
rm {output.merged_gtf}.temp
        """

rule tabixindex:
    input:
        annot = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}"
    output:
        gz = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.gz",
        index = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.gz.tbi"
    benchmark:
        "aref/benchmarks/tabixindex/{sample_or_ref}.{annot}_{default_or_extended}.tsv"
    resources:
        mem_mb = 60000,
        runtime = 300
    conda:
        "omics"
    shell:
        """

awk '!/#/ {{print}}' {input.annot} | sort -k1,1V -k4,4n -k5,5n -t '\t'| bgzip > {input.annot}.gz
tabix -p gff {input.annot}.gz
        """

rule get2bitgenome:
    input:
        ref = "aref/{default_or_extended}/A.REF.fa"
    output:
        genome2bit = "aref/{default_or_extended}/A.REF.2bit"
    benchmark:
        "aref/benchmarks/get2bitgenome/get2bitgenome_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
faToTwoBit {input.ref} {output.genome2bit}
        """


rule makeTxDB:
    input:
        refseq = "aref/default/A.REF_annotations/refseq.gff3",
        repeatmasker = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.gff3",
        genome2bit = "aref/{default_or_extended}/A.REF.2bit"
    output:
        txdb = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.sqlite",
        txdbrefseq = "aref/{default_or_extended}/A.REF_annotations/refseq.sqlite",
        txdbrepeatmasker = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.sqlite"
    benchmark:
        "aref/benchmarks/makeTxDB/makeTxDB_{default_or_extended}.tsv"
    resources:
        mem_mb = 120000
    conda:
        "repeatanalysis"
    script:
        "scripts/txdbBSgenome.R"

rule get_transcriptome:
    input:
        gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.gtf",
        ref = "aref/{default_or_extended}/A.REF.fa"
    output:
        fa = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.fa"
    benchmark:
        "aref/benchmarks/get_transcriptome/{sample_or_ref}.{annot}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_sp_extract_sequences.pl -g {input.gtf} -f {input.ref} -t exon --merge -o {output.fa}
        """

rule annotate_rtes:
    input:
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        ref = "aref/{default_or_extended}/{sample_or_ref}.fa",
        fai = "aref/{default_or_extended}/{sample_or_ref}.fa.fai",
        txdbrefseq = "aref/default/A.REF_annotations/refseq.sqlite",
    output:
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        rmann = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rmann.csv",
        rmann_nonref = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rmann_nonref.csv"
    benchmark:
        "aref/benchmarks/annotate_rtes/{sample_or_ref}_{default_or_extended}.tsv"
    priority:
        100
    conda:
        "evo2"
    script:
        "scripts/annotate_rtes.R"

rule getRTEbeds:
    input:
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
    output:
        outfile = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt"
    benchmark:
        "aref/benchmarks/getRTEbeds/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/getRTEbeds.R"

rule element_analysis:
    input:
        r_annotation_fragmentsjoined = "aref/default/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/default/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/default/{sample_or_ref}.fa"
    params:
        l13 = config["aref"]["l13fasta"]
    output:
        plots = "aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds"
    benchmark:
        "aref/benchmarks/element_analysis/{sample_or_ref}.tsv"
    conda:
        "evo2"
    script:
        "scripts/element_analysis.R"


rule make_star_index:
    input:
        reference = "aref/{default_or_extended}/{sample_or_ref}.fa"
    output:
        outfile = "aref/{default_or_extended}/{sample_or_ref}_indeces/make_star_index.out"
    benchmark:
        "aref/benchmarks/make_star_index/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        cpus_per_task = 32,
        mem_mb = 128000,
        runtime = 300
    conda:
        "star"
    shell:
        """
mkdir -p $(dirname {output.outfile})
STAR --runThreadN  30 --runMode genomeGenerate --genomeDir $(dirname {output.outfile})/star_index --genomeFastaFiles {input.reference} --outTmpDir $(dirname {output.outfile})/tmp
touch {output.outfile}
        """

rule cpgIsland:
    params:
        ref_cpgislands = config["aref"]["ref_cpgislands"]
    output:
        cpg_islands_fullinfo = "aref/extended/A.REF_annotations/cpg_islands.tsv",
        cpg_islands = "aref/extended/A.REF_annotations/cpg_islands.bed",
        cpgi_shores = "aref/extended/A.REF_annotations/cpgi_shores.bed",
        cpgi_shelves = "aref/extended/A.REF_annotations/cpgi_shelves.bed"
    benchmark:
        "aref/benchmarks/cpgIsland/cpgIsland.tsv"
    priority:
        100
    resources:
        cpus_per_task = 2,
        mem_mb = 20000,
        runtime = 60
    conda:
        "ds"
    script:
        "scripts/cpgIsland.R"

rule copySelectAnnotations:
    params:
        ref_cytobands = config["aref"]["ref_cytobands"],
        ref_telomere = config["aref"]["ref_telomere"],
    output:
        ref_cytobands = "aref/{default_or_extended}/A.REF_annotations/cytobands.bed",
        ref_telomere = "aref/{default_or_extended}/A.REF_annotations/telomeres.bed",
    benchmark:
        "aref/benchmarks/copySelectAnnotations/copySelectAnnotations_{default_or_extended}.tsv"
    shell:
        """
cp {params.ref_cytobands} {output.ref_cytobands}
cp {params.ref_telomere} {output.ref_telomere}
        """


rule create_blast_db:
    input:
        ref = "aref/{default_or_extended}/{sample_or_ref}.fa",
    output:
        outfile = "aref/{default_or_extended}/blastdb/{sample_or_ref}.blastdb.outfile"
    benchmark:
        "aref/benchmarks/create_blast_db/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    shell:
        """
makeblastdb -in {input.ref} -dbtype nucl -out aref/{wildcards.default_or_extended}/blastdb/{wildcards.sample_or_ref}
touch {output.outfile}
        """

rule transduction_mapping:
    input:
        filtered_tldr = "aref/{default_or_extended}/{sample_or_ref}.table.kept_in_updated_ref.txt",
        unfiltered_tldr = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{default_or_extended}/{sample_or_ref}.fa",
        blast_njs = "aref/{default_or_extended}/blastdb/{sample_or_ref}.blastdb.outfile",
        element_analysis = "aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds"
    params:
        sample_or_ref = lambda w: w.sample_or_ref
    output:
        plots = "aref/{default_or_extended}/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds",
        # circlize_phylogeny = "aref/{sample_or_ref}_Analysis/tldr_plots/circlize_phylogeny.png",
        # circlize_transduction = "aref/{sample_or_ref}_Analysis/tldr_plots/circlize_transduction.png",
        # transduction_df = "aref/{sample_or_ref}_Analysis/transduction_df.csv"
    benchmark:
        "aref/benchmarks/transduction_mapping/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/transduction_mapping.R"


rule setup_protein_blast_db:
    output:
        outfile = "aref/blastdb/pdbaa/outfile.out"
    benchmark:
        "aref/benchmarks/setup_protein_blast_db/setup_protein_blast_db.tsv"
    conda:
        "blast"
    shell:
        """
mkdir -p $(dirname {output.outfile})
cd $(dirname {output.outfile})
update_blastdb.pl --decompress pdbaa
touch outfile.out
        """


rule extract_putative_translocation_supporting_reads:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
    output:
        alndata = "aref/intermediates/{sample}/translocation_analysis/putative_trans_reads.tsv",
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.alndata})
#exclude all secondary and supplementary alignments
samtools view -F 0x800 -F 0x100 {input.sortedbam} | bioawk -c sam '
{{
    count = 0
    #matching SA tag which denotes a read which has supplementary alignments
    match($0, /SA:Z[^ ]*/)
    if (RSTART != 0) {{
        savl = substr($0, RSTART, RLENGTH)
        count = gsub(/;/, "&", savl)
    }}
    #only keeping SAs which have only one supplementary portion
    if (count == 1) {{
        # Extract the relevant part of the string (the part after "SA:Z:")
        split(savl, b, ",")

        # Extract the CIGAR string (b[4])
        sacigar = b[4]

        # Initialize running sum
        running_sum = 0

        # Split the CIGAR string into components
        while (match(sacigar, /([0-9]+)M/)) {{
            num = substr(sacigar, RSTART, RLENGTH - 1)
            running_sum += num + 0
            sacigar = substr(sacigar, RSTART + RLENGTH)
        }}

        # only keep reads with supplementary portion with at least 500bp aligned
        if (running_sum > 500) {{
            print $qname, $flag, $rname, $pos, $mapq, $cigar, savl
        }}
    }}
}}
' > {output.alndata}
        """

rule get_all_read_clipping_lengths:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["aref"]["rate"], type = config["aref"]["type"], modification_string = config["aref"]["modification_string"]),
    output:
        alndata = "aref/intermediates/{sample}/translocation_analysis/read_clipping_lengths.tsv",
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.alndata})
#exclude all secondary and supplementary alignments
samtools view -F 0x800 -F 0x100 {input.sortedbam} | bioawk -c sam '
{{
    # Initialize clipping lengths
    start_clipping = 0
    end_clipping = 0

    # Check for clipping at the start
    if ($6 ~ /^([0-9]+[SH])/) {{
        split($6, a, /[SH]/)
        start_clipping = a[1]
    }}

    # Check for clipping at the end
    if ($6 ~ /([0-9]+[SH])$/) {{
        split($6, a, /[MIDSH]/)
        end_clipping = a[length(a)-1]
    }}

    {{
        print start_clipping, end_clipping
    }}
}}
' > {output.alndata}
        """

rule call_read_extractions:
    input:
        alndata = expand("aref/intermediates/{sample}/translocation_analysis/read_clipping_lengths.tsv", sample = config["aref"]["samples"]),
        alndata1 = expand("aref/intermediates/{sample}/translocation_analysis/putative_trans_reads.tsv", sample = config["aref"]["samples"])
    




#terex genome prep
rule lastdb_index:
    input:
        genome= config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"]
    output:
        db="aref/terex/gdb.prj"
    threads: 8
    conda: "last"
    shell:
        """
mkdir -p $(dirname {output.db})
db={output.db}
lastdb -P{threads} -uRY4 ${{db%.*}} {input.genome}
        """

rule download_repeats:
    output:
        "aref/terex/prep/reps.fa"
    params:
        terexbindir = "/users/mkelsey/data/tools/te-rex/bin"
    shell:
        """
mkdir -p $(dirname {output})
{params.terexbindir}/dfam-fasta -c9606 -x'root;Interspersed_Repeat' > {output}
        """

rule make_repdb:
    input:
        "aref/terex/prep/reps.fa"
    output:
        outfile = "aref/terex/prep/outfile.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda: "last"
    shell:
        """
mkdir -p $(dirname {output.outfile})
lastdb -uMAM8 -S2 -P24 $(dirname {output.outfile})/repdb {input}
touch {output.outfile}
        """

rule train_genome_to_reps:
    input:
        outfile = "aref/terex/prep/outfile.txt",
        genome= config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"]
    output:
        "aref/terex/prep/rep.train"
    conda: "last"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    shell:
        "last-train -P24 --revsym -X1 --pid=70 $(dirname {input.outfile})/repdb {input.genome} > {output}"

rule genome_to_reps:
    input:
        outfile = "aref/terex/prep/outfile.txt",
        train="aref/terex/prep/rep.train",
        genome= config["aref"]["update_ref_with_tldr"]["tldr_input_bam_ref"]
    output:
        "aref/terex/prep/genome-to-reps.maf"
    conda: "last"
    resources:
        cpus_per_task = 24,
        mem_mb = 200000,
        runtime = 300
    shell:
        """
lastal -P12 -p {input.train} -D1e7 --split $(dirname {input.outfile})/repdb {input.genome} | last-postmask > {output}
        """

rule reps_to_reps:
    input:
        repeats="aref/terex/prep/reps.fa"
    output:
        "aref/terex/prep/reps-to-reps.tab"
    params:
        terexbindir = "/users/mkelsey/data/tools/te-rex/bin"
    conda: "last"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    shell:
        """
{params.terexbindir}/te-self-align {params.terexbindir}/hg38-calJac3.train {input.repeats} > {output}
        """
#terex on my reads
rule last_train:
    input:
        db="aref/terex/gdb.prj",
        reads= expand("ldna/intermediates/{agivensample}/fastqs/{agivensample}.fq",agivensample=[config["aref"]["samples"][1] if len(config["aref"]["samples"]) > 1 else "default_sample"])
    output:
        params="aref/terex/reads.train"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda: "last"
    shell:
        """
db={input.db}
last-train -P24 -Q0 ${{db%.*}} {input.reads} > {output.params}
        """

rule lastal_align:
    input:
        db="aref/terex/gdb.prj",
        reads= "ldna/intermediates/{sample}/fastqs/{sample}.fq",
        params="aref/terex/reads.train"
    output:
        aligned="aref/terex/{sample}/{sample}.maf"
    conda: "last"
    resources:
        cpus_per_task = 14,
        mem_mb = 240000,
        runtime = 1200
    shell:
        """
mkdir -p $(dirname {output.aligned})
db={input.db}
lastal -P10 -p {input.params} -D1e9 --split ${{db%.*}} {input.reads} | last-postmask > {output.aligned}
        """

#pull everything together
rule te_rex:
    input:
        genome_to_reps="aref/terex/prep/genome-to-reps.maf",
        reps_to_reps="aref/terex/prep/reps-to-reps.tab",
        reads_maf="aref/terex/{sample}/{sample}.maf"
    params:
        terexbindir = "/users/mkelsey/data/tools/te-rex/bin"
    output:
        recomb_tsv="aref/terex/{sample}/{sample}-out-recomb.tsv",
        recomb_maf="aref/terex/{sample}/{sample}-out-recomb.maf",
        insert_maf="aref/terex/{sample}/{sample}-out-insert.maf"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 500
    conda: "last"
    shell:
        """
outfile={output.insert_maf}
{params.terexbindir}/te-rex {input.genome_to_reps} {input.reps_to_reps} {input.reads_maf} ${{outfile%-*}}
        """
rule call_te_rex:
    input:
        expand("aref/terex/{sample}/{sample}-out-insert.maf", sample = config["aref"]["samples"])