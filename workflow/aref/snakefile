container: "docker://maxfieldkelsey/pipelinerteseq:v1.0"
import os
import pandas as pd
import csv
from pathlib import Path
from pandas.core.common import flatten

sample_table = pd.read_csv("conf/sample_table_aref.csv")

if config["update_ref_with_tldr"]["response"] == "yes":
    if config["running_ldna"]["response"] == "yes":
        d_or_e = ["default", "extended"]
    else:
        d_or_e = ["default"] 
else:
    d_or_e = ["default"]

def aref_complete_input(wildcards):
    final_input = []
    if config["symlink_aref_contents"]["response"] == "yes":
        final_input.append("aref_contents.symlinked.outfile")
        return final_input

    if config["update_ref_with_tldr"]["response"] == "yes":
        if config["running_ldna"]["response"] == "yes":
            d_or_e = ["default", "extended"]
        else:
            d_or_e = ["default"] 
    else:
        d_or_e = ["default"]

    paths = [
        "aref/default/A.REF_Analysis/l1element_analysis.rds",
        "aref/default/A.REF_annotations/refseq.complete.gff3.gz.tbi",
        "aref/default/A.REF_annotations/refseq.complete.gtf.gz.tbi",
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.gff3.gz.tbi",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.gtf.gz.tbi",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gff3",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.bed",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_rte_beds/outfile.txt",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.sqlite",default_or_extended = d_or_e),
        expand("aref/{default_or_extended}/A.REF_annotations/cytobands.bed",default_or_extended = d_or_e),
    ]

    if config["update_ref_with_tldr"]["response"] == "yes":
        paths.append(expand("aref/qc/{sample_or_ref}/{sample_or_ref}pycoQC.html", sample_or_ref = config["samples"]))
        paths.append("aref/qc/multiqc.out")
        paths.append("aref/results/insertions/analyze_nongermline_insertions.rds")

        if config["update_ref_with_tldr"]["per_sample"] == "yes":
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds", sample_or_ref = config["samples"]))
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds", sample_or_ref = config["samples"]))
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds", sample_or_ref = config["samples"]))
            paths.append(expand("aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt", sample_or_ref = config["samples"]))
            
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt", sample_or_ref = config["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt", sample_or_ref = config["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gtf", sample_or_ref = config["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.gff3", sample_or_ref = config["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gff3", sample_or_ref = config["samples"], default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_indeces/make_star_index.out", sample_or_ref = config["samples"], default_or_extended = d_or_e))

        else:
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds", sample_or_ref = "A.REF"))
            paths.append(expand("aref/default/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds", sample_or_ref = "A.REF"))
            
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gtf", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.gff3", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gff3", sample_or_ref = "A.REF", default_or_extended = d_or_e))
            paths.append(expand("aref/{default_or_extended}/{sample_or_ref}_indeces/make_star_index.out", sample_or_ref = "A.REF", default_or_extended = d_or_e))

    else:
        paths.append("aref/default/A.REF_indeces/make_star_index.out")


    if config["running_ldna"]["response"] == "yes":
        paths.append("aref/extended/A.REF_annotations/cpg_islands.bed")
    
    for path in list(flatten(paths)):
        final_input.append(path)
    print(final_input)
    return final_input


#tag FILESTRUCTURE
paths = [
    "aref/benchmarks",
    ]
if config["symlink_aref"]["response"] == "no":
    for path in paths:
        os.makedirs(path, exist_ok = True)

rule all:
    input: ancient("aref.done.outfile")
rule aref_complete:
    input: aref_complete_input
    output: "aref.done.outfile"
    shell: "touch {output}"
rule sym_link:
    input:
        aref_dir = config["symlink_aref"]["aref_symlinkdir"]
    priority: 100
    output:
        "aref.done.outfile"
    shell:
        """
ln -s {input.aref_dir} aref
touch {output}
        """
rule sym_link_aref_contents:
    input:
        aref_dir = config["symlink_aref_contents"]["aref_symlinkdir"]
    output:
        sym_link_outfile = "aref_contents.symlinked.outfile"
    shell:
        """
ln -s {input.aref_dir}/* aref/
touch {output.sym_link_outfile}
        """

#tag BASECALLING
rule dorado:
    input:
        dir = lambda wildcards: sample_table.loc[sample_table["sample_name"] == wildcards.sample, "nanopore_rawdata_dir"].iloc[0]
    output:
        calls = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    container: None
    benchmark:
        "aref/benchmarks/dorado/{rate}.{sample}.{type}.{modifications_string}.tsv"
    params:
        dorado = config["dorado"],
        reference = config["update_ref_with_tldr"]["tldr_input_bam_ref"]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=96:00:00 --prefer=a6000 --gres=gpu:2"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{input.dir} \
--recursive \
--verbose \
--reference {params.reference} > {output.calls}.unfinished
mv {output.calls}.unfinished {output.calls}
        """

rule call_dorado_resume:
    input:
        expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.unfinished", sample = config["samples"], rate = config["rate"], type = config["type"], modifications_string = config["modification_string"])

rule dorado_resume:
    input:
        unfinished = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.unfinished"
    output:
        calls = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.finished"
    benchmark:
        "aref/benchmarks/dorado_resume/{rate}.{sample}.{type}.{modifications_string}.tsv"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    params:
        dorado = config["dorado"],
        basecallingModel = lambda w: config["basecallingModel"][w.rate][w.type],
        reference = config["update_ref_with_tldr"]["tldr_input_bam_ref"]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=48:00:00 --prefer=a6000 --gres=gpu:2"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{input.dir} \
--resume-from {input.unfinished} \
--recursive \
--verbose \
--reference {params.reference} > {output.calls}.temp
mv {output.calls}.temp {output.calls}
        """

rule dorado_seqsummary:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        sortedbamindex = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam.bai", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    params:
        dorado = config["dorado"]
    benchmark:
        "aref/benchmarks/dorado_seqsummary/{sample}.tsv"
    output:
        "aref/qc/{sample}/{sample}.doradosummary.txt"
    conda:
        "omics"
    shell:
        """
{params.dorado} summary {input.sortedbam} > {output}
        """

rule pycoQC:
    input:
        seqsummary = "aref/qc/{sample}/{sample}.doradosummary.txt",
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    output:
        html = "aref/qc/{sample}/{sample}pycoQC.html",
        json = "aref/qc/{sample}/{sample}pycoQC.json"
    benchmark:
        "aref/benchmarks/pycoQC/{sample}.tsv"
    resources:
        mem_mb = 100000,
    conda:
        "pycoQC"
    shell:
        """
mkdir -p $(dirname {output})
pycoQC --summary_file {input.seqsummary} --bam_file {input.sortedbam} --html_outfile {output.html} --json_outfile {output.json} --min_pass_qual 10
        """

rule multiqc:
    input:
        seqsummary = expand("aref/qc/{sample}/{sample}.doradosummary.txt", sample = config["samples"]),
        sortedbam = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam", sample = config["samples"], rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    output:
        mqc = "aref/qc/multiqc.out",
    benchmark:
        "aref/benchmarks/multiqc/multiqc.tsv"
    resources:
        mem_mb = 100000,
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.mqc})
multiqc -f -o $(dirname {output.mqc}) --export ./aref
touch {output.mqc}
        """



#tag ALIGNMENT_UTILITIES
rule subsample_bam:
    input:
        bam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    output:
        subsampled = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.subsampled.bam"
    benchmark:
        "aref/benchmarks/subsample_bam/{rate}.{sample}.{type}.{modifications_string}.tsv"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.subsampled})

samtools view -b {input.bam} chr10 > {output.subsampled}
        """
if config["update_ref_with_tldr"]["response"] == "yes":
    if config["update_ref_with_tldr"]["per_sample"] == "no":
        ruleorder: merge_bams_when_per_sample_is_no_for_ldna_variants > sortBam 
rule merge_bams_when_per_sample_is_no_for_ldna_variants:
    input:
        sorted_bams = expand("aref/intermediates/{sample}/alignments/{{rate}}/{sample}.{{type}}.{{modifications_string}}.sorted.bam", sample = config["samples"])
    output:
        merged_bams = "aref/intermediates/A.REF/alignments/{rate}/A.REF.{type}.{modifications_string}.sorted.bam"
    benchmark:
        "aref/benchmarks/sortBam/{rate}.{type}.{modifications_string}.tsv"
    wildcard_constraints:
        modifications_string = "[A-Za-z0-9_-]+"
    resources:
        cpus_per_task =10,
        mem_mb = 128000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output})
samtools merge -@8 {output.merged_bams} {input.sorted_bams}
        """

rule sortBam:
    input:
        bam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    output:
        sortedbam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.sorted.bam"
    benchmark:
        "aref/benchmarks/sortBam/{rate}.{sample}.{type}.{modifications_string}.tsv"
    wildcard_constraints:
        modifications_string = "[A-Za-z0-9_-]+"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output})
samtools sort -@8 -m4g {input.bam} > {output.sortedbam}
        """

rule IndexBam:
    input:
        bam = "aref/{bampath}.bam"
    output:
        index = "aref/{bampath}.bam.bai"
    benchmark:
        "aref/benchmarks/IndexBam/{bampath}.tsv"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
samtools index  -@6 {input.bam}
        """
        
species = config["species"]
rule tldr_aggregate_multiple_samples:
    input:
        bams = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam", sample = config["samples"], rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        bais = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam.bai", sample = config["samples"], rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    params:
        tldr_input_bam_ref = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldr_te_ref = lambda w: config["update_ref_with_tldr"]["tldr_te_ref"][species],
        known_nonref = lambda w: [config["update_ref_with_tldr"]["known_nonref"]["path"] if config["update_ref_with_tldr"]["known_nonref"]["response"] == "yes" else "not_provided"][0]
    benchmark:
        "aref/benchmarks/tldr_aggregate_multiple_samples/tldr_aggregate_multiple_samples.tsv"
    output:
        tldr = "aref/A.REF_tldr/A.REF.table.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda:
        "tldr"
    shell:
        """
bams=$(echo {input.bams})
commabams=$(echo $bams | tr ' ' ',')

nonrefavailible={params.known_nonref}
if [[ $nonrefavailible = "not_provided"]]; then

tldr -b $commabams \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--minreads 1 \
--outbase A.REF \
--detail_output \
--extend_consensus 4000 \
--trdcol

else

tldr -b $commabams \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--nonref {params.known_nonref} \
--minreads 1 \
--outbase A.REF \
--detail_output \
--extend_consensus 4000 \
--trdcol

fi

mkdir -p $(dirname {output.tldr})
mv A.REF.table.txt {output.tldr}
mv A.REF $(dirname {output.tldr})/
        """

rule call_tldr_per_sample:
    input:
        expand("aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt", sample = config["samples"], sample_or_ref = config["samples"])
rule tldr_per_sample:
    input:
        bam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        bai = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam.bai", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    benchmark:
        "aref/benchmarks/tldr_per_sample/{sample}.tsv"
    params:
        tldr_input_bam_ref = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldr_te_ref = lambda w: config["update_ref_with_tldr"]["tldr_te_ref"][species],
        known_nonref = lambda w: [config["update_ref_with_tldr"]["known_nonref"]["path"] if config["update_ref_with_tldr"]["known_nonref"]["response"] == "yes" else "not_provided"][0]
    output:
        tldr = "aref/{sample}_tldr/{sample}.table.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda:
        "tldr"
    shell:
        """
nonrefavailible={params.known_nonref}
if [[ $nonrefavailible = "not_provided"]]; then

tldr -b {input.bam} \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--minreads 1 \
--outbase {wildcards.sample} \
--detail_output \
--extend_consensus 4000 \
--trdcol

else

tldr -b {input.bam} \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--nonref {params.known_nonref} \
--minreads 1 \
--outbase {wildcards.sample} \
--detail_output \
--extend_consensus 4000 \
--trdcol

fi

mkdir -p $(dirname {output.tldr})
mv {wildcards.sample}.table.txt {output.tldr}
mv {wildcards.sample} $(dirname {output.tldr})/
        """

rule update_reference:
    input:
        reference = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        updated_reference = "aref/default/{sample_or_ref}-pre-ins-filtering.fa",
        non_ref_contigs = "aref/default/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa",
    benchmark:
        "aref/benchmarks/update_reference/{sample_or_ref}.tsv"
    conda:
        "repeatanalysis"
    script:
        "scripts/create_reference.R"

rule update_reference_extended_nonref_flank:
    input:
        reference = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        updated_reference_plusflank = "aref/extended/{sample_or_ref}-pre-ins-filtering.fa",
        non_ref_contigs_plusflank = "aref/extended/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa"
    benchmark:
        "aref/benchmarks/update_reference/{sample_or_ref}.tsv"
    conda:
        "repeatanalysis"
    script:
        "scripts/create_reference_extended_nonref_flank.R"

rule copy_starting_reference:
    params:
        ref = config["starting_ref"]
    conda:
        "omics"
    priority: 100
    output:
        ref = "aref/{default_or_extended}/A.REF.fa"
    shell:
        """
cp {params.ref} {output.ref}
samtools faidx {output.ref}
        """

rule get_repeatmasker_raw:
    input:
        out = "aref/{default_or_extended}/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    output:
        repeatmasker = "aref/{default_or_extended}/{sample_or_ref}_repeatmasker/A.REF_repeatmasker_raw.gtf"
    benchmark:
        "aref/benchmarks/get_repeatmasker_raw/{sample_or_ref}_{default_or_extended}.tsv"
    shell:
        """
cp {input.out} {output.repeatmasker}
workflow/aref/scripts/outToGtf.sh {input.out} {output.repeatmasker}.unsorted
sort -k1,1V -k4,4n -k5,5n {output.repeatmasker}.unsorted > {output.repeatmasker}
rm {output.repeatmasker}.unsorted
        """

checkpoint index_reference:
    input:
        reference = "aref/{default_or_extended}/{sample_or_ref}.fa"
    output:
        reference_index = "aref/{default_or_extended}/{sample_or_ref}.fa.fai"
    benchmark:
        "aref/benchmarks/index_reference/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
samtools faidx {input.reference}
        """

#### to repeat mask or not to repeatmask, that is the question
rule get_repeatmasker_raw_out:
    params:
        rmref = config["run_repeatmasker"]["starting_ref_repeatmasker"]
    output:
        out = "aref/{default_or_extended}/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    shell:
        """
cp {params.rmref} {output.out}
        """

rule split_genome_fa_into_chr:
    input: 
        fasta = "aref/default/A.REF.fa",
        fai = "aref/default/A.REF.fa.fai"
    output:
        split = "aref/default/A.REF_split/A.REF_{chromosome}.fa"
    benchmark:
        "aref/benchmarks/split_genome_fa_into_chr/split_genome_fa_into_chr.{chromosome}.tsv"
    conda: "omics"
    shell:
        """
echo {wildcards.chromosome} > {output.split}.names.txt
seqtk subseq {input.fasta} {output.split}.names.txt > {output.split}
        """

rule repeatmasker_whole_genome:
    input: 
        split = "aref/default/A.REF_split/A.REF_{chromosome}.fa"
    params:
        species = config["species"]
    benchmark:
        "aref/benchmarks/repeatmasker_whole_genome/repeatmasker_whole_genome.{chromosome}.tsv"
    output:
        rmout = "aref/default/A.REF_repeatmasker/wholegenome/A.REF_{chromosome}.fa.out"
    resources:
        cpus_per_task =20,
        runtime = 1000,
        mem_mb = 30000
    container: "docker://dfam/tetools:1.88.5"
    shell:
        """
if [[ {params.species} == mouse ]]
then
species="Mus musculus"
else
species={params.species}
fi
mkdir -p $(dirname {output})
RepeatMasker -species "$species" -pa {resources.cpus_per_task} -gff {input.split} -dir $(dirname {output})
        """

def merge_outs_wholegenome_input(wildcards):
    #call checkpoint output just to register rule merge_rm_out_wholegenome as dependent on the move_staring_reference rule
    checkpoint_output = checkpoints.index_reference.get().output[0]
    if os.path.exists("aref/default/A.REF.fa.fai"):
        return expand("aref/default/A.REF_repeatmasker/wholegenome/A.REF_{chromosome}.fa.out", chromosome = pd.read_table("aref/default/A.REF.fa.fai", header = None)[0].tolist())
    else:
        return "TEMP"
rule merge_rm_out_wholegenome:
    input:
        outs = merge_outs_wholegenome_input
    benchmark:
        "aref/benchmarks/merge_rm_out_wholegenome/merge_rm_out_wholegenome.tsv"
    output:
        out = "aref/default/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    priority: 99
    conda:
        "evo2"
    shell:
        """
awk '{{ if (NR > 3) exit }} \
    NR < 4' {input.outs} > {output.out}
awk 'FNR > 3' {input.outs} >> {output.out}
        """


#######
#non ref contig buisness
def repeatmasker_input(wildcards):
    if config["update_ref_with_tldr"]["response"] == "yes":
        return expand("aref/{default_or_extended}/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa", sample_or_ref = wildcards.sample_or_ref, default_or_extended = wildcards.default_or_extended)
    else:
        return config["ref"]

rule repeatmasker:
    input:
        fasta = repeatmasker_input
    params:
        species = config["species"]
    benchmark:
        "aref/benchmarks/repeatmasker/{sample_or_ref}_{default_or_extended}.tsv"
    output:
        rmout = "aref/{default_or_extended}/A.REF_repeatmasker/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa.out"
    resources:
        cpus_per_task =20,
        runtime = 1200,
        mem_mb = 50000
    container: "docker://dfam/tetools:1.88.5"
    shell:
        """
if [[ {params.species} == mouse ]]
then
species="Mus musculus"
else
species={params.species}
fi
mkdir -p $(dirname {output})
RepeatMasker -species "$species" -pa {resources.cpus_per_task} -gff {input.fasta} -dir $(dirname {output})
        """

rule getGtfs:
    input:
        rmout = "aref/{default_or_extended}/A.REF_repeatmasker/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa.out",
        rmref = "aref/default/A.REF_repeatmasker/A.REF.repeatmasker_raw.out"
    params:
        module_prefix = config["prefix"]
    benchmark:
        "aref/benchmarks/getGtfs/{sample_or_ref}_{default_or_extended}.tsv"
    output:
       ref = "aref/{default_or_extended}/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_ref_raw.gtf",
       nonref = "aref/{default_or_extended}/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_nonref_raw.gtf",
       merged = "aref/{default_or_extended}/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_raw.gtf"
    conda:
        "evo2"
    shell:
        """
workflow/{params.module_prefix}/scripts/outToGtf.sh {input.rmout} {output.nonref}
workflow/{params.module_prefix}/scripts/outToGtf.sh {input.rmref} {output.ref}
cat {output.ref} {output.nonref} > {output.merged}
        """

rule process_gtf:
    input:
        gtf = "aref/{default_or_extended}/A.REF_repeatmasker/A.REF_repeatmasker_raw.gtf",
        ref_cytobands = "aref/{default_or_extended}/A.REF_annotations/cytobands.bed",
        ref = "aref/{default_or_extended}/A.REF.fa",
        reffai = "aref/{default_or_extended}/A.REF.fa.fai"
    output:
        repmask_gff2 = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.gff2",
        repmask_gff3 = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.gff3",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.gtf.rformatted.fragmentsjoined.csv"
    benchmark:
        "aref/benchmarks/process_gtf/process_gtf_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/process_gtf_tldr.R"

rule process_gtf_tldr:
    input:
        gtf = "aref/{default_or_extended}/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_raw.gtf",
        ref_cytobands = "aref/{default_or_extended}/A.REF_annotations/cytobands.bed",
        ref = "aref/{default_or_extended}/{sample_or_ref}-pre-ins-filtering.fa",
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        contigs_to_keep = "aref/{default_or_extended}/{sample_or_ref}_contigs_to_keep.txt",
        filtered_tldr = "aref/{default_or_extended}/{sample_or_ref}.table.kept_in_updated_ref.txt",
        repmask_gff2 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff2",
        repmask_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff3",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv"
    benchmark:
        "aref/benchmarks/process_gtf_tldr/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/process_gtf_tldr.R"

rule analyze_insertions:
    input:
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt",
        filtered_tldr = "aref/{default_or_extended}/{sample_or_ref}.table.kept_in_updated_ref.txt",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{default_or_extended}/{sample_or_ref}-pre-ins-filtering.fa",
        element_analysis = "aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds"
    output:
        plots = "aref/{default_or_extended}/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds",
    benchmark:
        "aref/benchmarks/analyze_insertions/{sample_or_ref}_{default_or_extended}/.tsv"
    conda:
        "evo2"
    script:
        "scripts/analyze_insertions.R"

rule analyze_nongermline_insertions:
    input:
        json = expand("aref/qc/{sample}/{sample}pycoQC.json", sample = config["samples"]),
        tldroutput = lambda w: expand("aref/{sample}_tldr/{sample}.table.txt", sample = config["samples"]) if config["update_ref_with_tldr"]["per_sample"] == "yes" else "aref/A.REF_tldr/A.REF.table.txt"
    output:
        plots = "aref/results/insertions/analyze_nongermline_insertions.rds"
    benchmark:
        "aref/benchmarks/analyze_nongermline_insertions/analyze_nongermline_insertions.tsv"
    conda:
        "ds"
    script:
        "scripts/analyze_nongermline_insertions.R"
        
rule cleanup_updated_ref:
    input:
        updated_reference = "aref/{default_or_extended}/A.REF-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{default_or_extended}/A.REF_contigs_to_keep.txt"
    output:
        filtered_ref = "aref/{default_or_extended}/A.REF.fa"
    benchmark:
        "aref/benchmarks/cleanup_updated_ref/cleanup_updated_ref_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
        """

rule cleanup_updated_sample_ref:
    input:
        updated_reference = "aref/{default_or_extended}/{sample}-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{default_or_extended}/{sample}_contigs_to_keep.txt"
    output:
        filtered_ref = "aref/{default_or_extended}/{sample}.fa"
    benchmark:
        "aref/benchmarks/cleanup_updated_sample_ref/{sample}_{default_or_extended}.tsv"
    wildcard_constraints:
        sample = "[A-Za-z0-9_]+"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
samtools faidx {output.filtered_ref}
        """

rule move_refseq:
    input:
        ref_refseq_gff3 = config["ref_refseq_gff3"],
        ref_refseq_gtf = config["ref_refseq_gtf"]
    output:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gtf"
    benchmark:
        "aref/benchmarks/move_refseq/{sample_or_ref}_{default_or_extended}.tsv"
    shell:
        """
cp {input.ref_refseq_gff3} {output.refseq_gff3}
cp {input.ref_refseq_gtf} {output.refseq_gtf}
        """

rule sort_refseq_get_bed:
    input:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.possibly_unsorted.gtf"
    output:
        refseq_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.gff3",
        refseq_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.gtf",
        refseq_bed = "aref/{default_or_extended}/{sample_or_ref}_annotations/refseq.bed"
    benchmark:
        "aref/benchmarks/sort_refseq_get_bed/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
awk '!/#/ {{print}}' {input.refseq_gff3} | sort -k1,1V -k4,4n -k5,5n > {output.refseq_gff3}
awk '!/#/ {{print}}' {input.refseq_gtf} | sort -k1,1V -k4,4n -k5,5n > {output.refseq_gtf}
gtf2bed < {output.refseq_gtf} > {output.refseq_bed}
        """

rule complete_gff3:
    input:
        gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.gff3",
    output:
        complete_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    benchmark:
        "aref/benchmarks/complete_gff3/{sample_or_ref}.{annotation}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
rm -f {output.complete_gff3}.temp
agat_convert_sp_gxf2gxf.pl --gff {input.gff3} -o {output.complete_gff3}.temp
awk '!/#/ {{print}}' {output.complete_gff3}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.complete_gff3}
rm  {output.complete_gff3}.temp
        """


rule gff_to_gtf:
    input:
        complete_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    output:
        gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gtf",
    benchmark:
        "aref/benchmarks/gff_to_gtf/{sample_or_ref}.{annotation}_{default_or_extended}.tsv"
    conda:
        "omics"
    resources:
        mem_mb = 128000,
        runtime = 300
    shell:
        """
rm -f {output.gtf}.temp
agat_convert_sp_gff2gtf.pl --gff {input.complete_gff3} -o {output.gtf}.temp
awk '!/#/ {{print}}' {output.gtf}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.gtf}
rm {output.gtf}.temp
        """

rule gff_to_bed12:
    input:
        complete_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    output:
        bed12 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annotation}.complete.bed"
    benchmark:
        "aref/benchmarks/gff_to_bed12/{sample_or_ref}.{annotation}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
rm -f {output.bed12}.temp
agat_convert_sp_gff2bed.pl --gff {input.complete_gff3} -o {output.bed12}.temp
awk '!/#/ {{print}}' {output.bed12}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k2,2n -k3,3n > {output.bed12}
rm {output.bed12}.temp
        """
if config["symlink_aref"]["response"] == "no":
    ruleorder: merge_genes_and_repeats_gff > complete_gff3 
rule merge_genes_and_repeats_gff:
    input:
        complete_repeat_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.complete.gff3",
        complete_refseq_gff3 = "aref/default/A.REF_annotations/refseq.complete.gff3"
    output:
        merged_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gff3"
    benchmark:
        "aref/benchmarks/merge_genes_and_repeats_gff/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 600
    conda:
        "omics"
    shell:
        """
rm -f {output.merged_gff3}.temp
agat_sp_merge_annotations.pl -f {input.complete_refseq_gff3} -f {input.complete_repeat_gff3} -o {output.merged_gff3}.temp
awk '!/#/ {{print}}' {output.merged_gff3}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n >  {output.merged_gff3}
rm {output.merged_gff3}.temp
        """

rule merge_OG_genes_and_repeats_gff3:
    input:
        repeatmasker = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff3",
        refseq = "aref/default/A.REF_annotations/refseq.gff3"
    output:
        merged_gff3 = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.gff3"
    benchmark:
        "aref/benchmarks/merge_OG_genes_and_repeats_gff3/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
rm -f {output.merged_gff3}.temp
awk '!/#/ {{print}}' {input.repeatmasker} | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' > {output.merged_gff3}.temp
awk '!/#/ {{print}}' {input.refseq} | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' >> {output.merged_gff3}.temp
cat {output.merged_gff3}.temp | sort -k1,1V -k4,4n -k5,5n > {output.merged_gff3}
rm {output.merged_gff3}.temp
        """

if config["symlink_aref"]["response"] == "no":
    ruleorder: merge_genes_and_repeats_gtf > gff_to_gtf 

rule merge_genes_and_repeats_gtf:
    input:
        complete_repeat_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.complete.gtf",
        complete_refseq_gtf = "aref/default/A.REF_annotations/refseq.complete.gtf"
    output:
        merged_gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_refseq.complete.gtf"
    benchmark:
        "aref/benchmarks/merge_genes_and_repeats_gtf/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 600
    conda:
        "omics"
    shell:
        """
rm -f {output.merged_gtf}.temp
agat_sp_merge_annotations.pl -f {input.complete_refseq_gtf} -f {input.complete_repeat_gtf} -o {output.merged_gtf}.temp
awk '!/#/ {{print}}' {output.merged_gtf}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.merged_gtf}
rm {output.merged_gtf}.temp
        """

rule tabixindex:
    input:
        annot = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}"
    output:
        gz = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.gz",
        index = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.gz.tbi"
    benchmark:
        "aref/benchmarks/tabixindex/{sample_or_ref}.{annot}_{default_or_extended}.tsv"
    resources:
        mem_mb = 60000,
        runtime = 300
    conda:
        "omics"
    shell:
        """

awk '!/#/ {{print}}' {input.annot} | sort -k1,1V -k4,4n -k5,5n -t '\t'| bgzip > {input.annot}.gz
tabix -p gff {input.annot}.gz
        """

rule get2bitgenome:
    input:
        ref = "aref/{default_or_extended}/A.REF.fa"
    output:
        genome2bit = "aref/{default_or_extended}/A.REF.2bit"
    benchmark:
        "aref/benchmarks/get2bitgenome/get2bitgenome_{default_or_extended}.tsv"
    conda:
        "omics"
    shell:
        """
faToTwoBit {input.ref} {output.genome2bit}
        """


rule makeTxDB:
    input:
        refseq = "aref/default/A.REF_annotations/refseq.gff3",
        repeatmasker = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.gff3",
        genome2bit = "aref/{default_or_extended}/A.REF.2bit"
    output:
        txdb = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker_refseq.complete.sqlite",
        txdbrefseq = "aref/{default_or_extended}/A.REF_annotations/refseq.sqlite",
        txdbrepeatmasker = "aref/{default_or_extended}/A.REF_annotations/A.REF_repeatmasker.complete.sqlite"
    benchmark:
        "aref/benchmarks/makeTxDB/makeTxDB_{default_or_extended}.tsv"
    resources:
        mem_mb = 120000
    conda:
        "repeatanalysis"
    script:
        "scripts/txdbBSgenome.R"

rule get_transcriptome:
    input:
        gtf = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.gtf",
        ref = "aref/{default_or_extended}/A.REF.fa"
    output:
        fa = "aref/{default_or_extended}/{sample_or_ref}_annotations/{annot}.fa"
    benchmark:
        "aref/benchmarks/get_transcriptome/{sample_or_ref}.{annot}_{default_or_extended}.tsv"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_sp_extract_sequences.pl -g {input.gtf} -f {input.ref} -t exon --merge -o {output.fa}
        """

rule annotate_rtes:
    input:
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        ref = "aref/{default_or_extended}/{sample_or_ref}.fa",
        fai = "aref/{default_or_extended}/{sample_or_ref}.fa.fai",
        txdbrefseq = "aref/default/A.REF_annotations/refseq.sqlite",
    output:
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        rmann = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rmann.csv",
        rmann_nonref = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rmann_nonref.csv"
    benchmark:
        "aref/benchmarks/annotate_rtes/{sample_or_ref}_{default_or_extended}.tsv"
    priority:
        100
    conda:
        "evo2"
    script:
        "scripts/annotate_rtes.R"

rule getRTEbeds:
    input:
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
    output:
        outfile = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt"
    benchmark:
        "aref/benchmarks/getRTEbeds/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/getRTEbeds.R"

rule element_analysis:
    input:
        r_annotation_fragmentsjoined = "aref/default/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/default/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/default/{sample_or_ref}.fa"
    params:
        l13 = config["l13fasta"]
    output:
        plots = "aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds"
    benchmark:
        "aref/benchmarks/element_analysis/{sample_or_ref}.tsv"
    conda:
        "evo2"
    script:
        "scripts/element_analysis.R"


rule make_star_index:
    input:
        reference = "aref/{default_or_extended}/{sample_or_ref}.fa"
    output:
        outfile = "aref/{default_or_extended}/{sample_or_ref}_indeces/make_star_index.out"
    benchmark:
        "aref/benchmarks/make_star_index/{sample_or_ref}_{default_or_extended}.tsv"
    resources:
        cpus_per_task = 32,
        mem_mb = 128000,
        runtime = 300
    conda:
        "star"
    shell:
        """
mkdir -p $(dirname {output.outfile})
STAR --runThreadN  30 --runMode genomeGenerate --genomeDir $(dirname {output.outfile})/star_index --genomeFastaFiles {input.reference} --outTmpDir $(dirname {output.outfile})/tmp
touch {output.outfile}
        """

rule cpgIsland:
    params:
        ref_cpgislands = config["ref_cpgislands"]
    output:
        cpg_islands_fullinfo = "aref/extended/A.REF_annotations/cpg_islands.tsv",
        cpg_islands = "aref/extended/A.REF_annotations/cpg_islands.bed",
        cpgi_shores = "aref/extended/A.REF_annotations/cpgi_shores.bed",
        cpgi_shelves = "aref/extended/A.REF_annotations/cpgi_shelves.bed"
    benchmark:
        "aref/benchmarks/cpgIsland/cpgIsland.tsv"
    priority:
        100
    resources:
        cpus_per_task = 2,
        mem_mb = 20000,
        runtime = 60
    conda:
        "ds"
    script:
        "scripts/cpgIsland.R"

rule copySelectAnnotations:
    params:
        ref_cytobands = config["ref_cytobands"],
        ref_telomere = config["ref_telomere"],
    output:
        ref_cytobands = "aref/{default_or_extended}/A.REF_annotations/cytobands.bed",
        ref_telomere = "aref/{default_or_extended}/A.REF_annotations/telomeres.bed",
    benchmark:
        "aref/benchmarks/copySelectAnnotations/copySelectAnnotations_{default_or_extended}.tsv"
    shell:
        """
cp {params.ref_cytobands} {output.ref_cytobands}
cp {params.ref_telomere} {output.ref_telomere}
        """


rule create_blast_db:
    input:
        ref = "aref/{default_or_extended}/{sample_or_ref}.fa",
    output:
        outfile = "aref/{default_or_extended}/blastdb/{sample_or_ref}.blastdb.outfile"
    benchmark:
        "aref/benchmarks/create_blast_db/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    shell:
        """
makeblastdb -in {input.ref} -dbtype nucl -out aref/{wildcards.default_or_extended}/blastdb/{wildcards.sample_or_ref}
touch {output.outfile}
        """

rule transduction_mapping:
    input:
        filtered_tldr = "aref/{default_or_extended}/{sample_or_ref}.table.kept_in_updated_ref.txt",
        unfiltered_tldr = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt",
        r_annotation_fragmentsjoined = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{default_or_extended}/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{default_or_extended}/{sample_or_ref}.fa",
        blast_njs = "aref/{default_or_extended}/blastdb/{sample_or_ref}.blastdb.outfile",
        element_analysis = "aref/default/{sample_or_ref}_Analysis/l1element_analysis.rds"
    params:
        sample_or_ref = lambda w: w.sample_or_ref
    output:
        plots = "aref/{default_or_extended}/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds",
        # circlize_phylogeny = "aref/{sample_or_ref}_Analysis/tldr_plots/circlize_phylogeny.png",
        # circlize_transduction = "aref/{sample_or_ref}_Analysis/tldr_plots/circlize_transduction.png",
        # transduction_df = "aref/{sample_or_ref}_Analysis/transduction_df.csv"
    benchmark:
        "aref/benchmarks/transduction_mapping/{sample_or_ref}_{default_or_extended}.tsv"
    conda:
        "evo2"
    script:
        "scripts/transduction_mapping.R"


rule setup_protein_blast_db:
    output:
        outfile = "aref/blastdb/pdbaa/outfile.out"
    benchmark:
        "aref/benchmarks/setup_protein_blast_db/setup_protein_blast_db.tsv"
    conda:
        "blast"
    shell:
        """
mkdir -p $(dirname {output.outfile})
cd $(dirname {output.outfile})
update_blastdb.pl --decompress pdbaa
touch outfile.out
        """


rule extract_putative_translocation_supporting_reads:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    output:
        alndata = "aref/intermediates/{sample}/translocation_analysis/putative_trans_reads.tsv",
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.alndata})
#exclude all secondary and supplementary alignments
samtools view -F 0x800 -F 0x100 {input.sortedbam} | bioawk -c sam '
{{
    count = 0
    #matching SA tag which denotes a read which has supplementary alignments
    match($0, /SA:Z[^ ]*/)
    if (RSTART != 0) {{
        savl = substr($0, RSTART, RLENGTH)
        count = gsub(/;/, "&", savl)
    }}
    #only keeping SAs which have only one supplementary portion
    if (count == 1) {{
        # Extract the relevant part of the string (the part after "SA:Z:")
        split(savl, b, ",")

        # Extract the CIGAR string (b[4])
        sacigar = b[4]

        # Initialize running sum
        running_sum = 0

        # Split the CIGAR string into components
        while (match(sacigar, /([0-9]+)M/)) {{
            num = substr(sacigar, RSTART, RLENGTH - 1)
            running_sum += num + 0
            sacigar = substr(sacigar, RSTART + RLENGTH)
        }}

        # only keep reads with supplementary portion with at least 500bp aligned
        if (running_sum > 500) {{
            print $qname, $flag, $rname, $pos, $mapq, $cigar, savl
        }}
    }}
}}
' > {output.alndata}
        """

rule get_all_read_clipping_lengths:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    output:
        alndata = "aref/intermediates/{sample}/translocation_analysis/read_clipping_lengths.tsv",
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output.alndata})
#exclude all secondary and supplementary alignments
samtools view -F 0x800 -F 0x100 {input.sortedbam} | bioawk -c sam '
{{
    # Initialize clipping lengths
    start_clipping = 0
    end_clipping = 0

    # Check for clipping at the start
    if ($6 ~ /^([0-9]+[SH])/) {{
        split($6, a, /[SH]/)
        start_clipping = a[1]
    }}

    # Check for clipping at the end
    if ($6 ~ /([0-9]+[SH])$/) {{
        split($6, a, /[MIDSH]/)
        end_clipping = a[length(a)-1]
    }}

    {{
        print start_clipping, end_clipping
    }}
}}
' > {output.alndata}
        """

rule call_read_extractions:
    input:
        alndata = expand("aref/intermediates/{sample}/translocation_analysis/read_clipping_lengths.tsv", sample = config["samples"]),
        alndata1 = expand("aref/intermediates/{sample}/translocation_analysis/putative_trans_reads.tsv", sample = config["samples"])
    